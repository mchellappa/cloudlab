{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFEUuGXNNSGB"
   },
   "source": [
    "# RNN (Graded)\n",
    "\n",
    "Welcome to your RNN (required) programming assignment! You will build a **Recurrent Neural Network(RNN)** for **text completion** task. You will be using [WikiText language modeling dataset](https://huggingface.co/datasets/Salesforce/wikitext) which contains million of tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "\n",
    "**Instructions:**\n",
    "* Do not modify any of the codes.\n",
    "* Only write code when prompted. For example in some sections you will find the following,\n",
    "```\n",
    "# TODO\n",
    "```\n",
    "Only modify those sections of the code.\n",
    "And follow the instructions in the code cell where you need to write code.\n",
    "\n",
    "**You will learn to:**\n",
    "* Explore the [WikiText language modeling dataset](https://huggingface.co/datasets/Salesforce/wikitext) dataset.\n",
    "* Clean the dataset before using it for training.\n",
    "* Build a robust text completion model using just `SimpleRNN()`.\n",
    "* Build the general architecture of a RNN, including:\n",
    "  * Initializing parameters\n",
    "  * Calculating the cost function and its gradient\n",
    "  * Using an optimization algorithm (gradient descent)\n",
    "* Gather all three functions above into a main model function, in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x3zjsreV0a0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from helpers import *\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx9CPRkQGweR"
   },
   "source": [
    "# Loading and Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAcdiWdOHPLD"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFTG-DSKQkZm"
   },
   "outputs": [],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYIq6yytK-M7"
   },
   "outputs": [],
   "source": [
    "train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyjpARImgGbS"
   },
   "source": [
    "<center>\n",
    "<h1>Preparing Dataset for Text Completion</h1>\n",
    "</center>\n",
    "\n",
    "There are essentially 3 steps to achieve text completion.\n",
    "\n",
    "\n",
    "### 1. Start with a sentence\n",
    "Take your sentence and break it into tokens (or words), for example:\n",
    "```\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "```\n",
    "### 2. Create progressive input output pairs\n",
    "* Start with just the first word and make the next word the \"output\" you're trying to predict:\n",
    "```\n",
    "Input: [\"The\"] → Output: \"cat\"\n",
    "```\n",
    "* Now, add one more word to the input and make the next word the output:\n",
    "```\n",
    "Input: [\"The\", \"cat\"] → Output: \"sat\"\n",
    "```\n",
    "* Keep adding more words to the input until you've gone through the sentence:\n",
    "```\n",
    "Input: [\"The\", \"cat\", \"sat\"] → Output: \"on\"\n",
    "Input: [\"The\", \"cat\", \"sat\", \"on\"] → Output: \"the\"\n",
    "Input: [\"The\", \"cat\", \"sat\", \"on\", \"the\"] → Output: \"mat\"\n",
    "```\n",
    "\n",
    "### 3. Pad the input to a fixed length\n",
    "To make all the inputs the same length (because some sentences might be shorter or longer), you \"pad\" the inputs by adding zeros at the beginning. For example, if you want every input to be 6 words long, the input would look like this:\n",
    "```\n",
    "Input: [0, 0, 0, \"The\"] → Output: \"cat\"\n",
    "Input: [0, 0, \"The\", \"cat\"] → Output: \"sat\"\n",
    "```\n",
    "This padding makes sure every input is the same size, which is important for training the model.\n",
    "\n",
    "### In Summary:\n",
    "* **Input**: You take a growing part of the sentence, starting small and getting bigger.\n",
    "* **Output**: The next word in the sentence.\n",
    "* **Padding**: If the input is too short, you add zeros at the start to make all inputs the same length.\n",
    "\n",
    "By doing this for each sentence in your dataset, you create many input-output pairs for the model to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOQen8mQG0Hx"
   },
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qye5TrrtLOeC"
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "As you can see, there are a lot of ambiguous characters such as:\n",
    "```\n",
    "\"= Valkyria Chronicles III =\\nSenjō, \"戦場のヴァルキュリア3\".\n",
    "```\n",
    "Its important to consider cleaning them.<br>\n",
    "Complete the following `clean_text()` method to build a function to clean the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kC18d3p16p-u"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to clean the dataset\n",
    "def clean_text(texts):\n",
    "    \"\"\"\n",
    "    Cleans the input text by performing necessary preprocessing steps like lowercasing,\n",
    "    removing special characters, etc.\n",
    "    \"\"\"\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        # Lowercase text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove headers/formatting (e.g., '= Valkyria Chronicles III =')\n",
    "        text = re.sub(r'=.*=', '', text)\n",
    "        \n",
    "        # Remove non-alphabetic characters (punctuation, numbers, special characters)\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Only keep non-empty cleaned lines\n",
    "        if text:\n",
    "            cleaned_texts.append(text)\n",
    "\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6loJfl53wfs0"
   },
   "source": [
    "\n",
    "## Train, val, test split\n",
    "Out of 23k training records, you are asked to **consider atleast 5k records for training**.\n",
    "\n",
    "* **Training samples:** ~5000\n",
    "\n",
    "* **Valid samples:** 60\n",
    "\n",
    "* **Test samples:** 62\n",
    "\n",
    "Adjust the following `train_samples` variable to select the number of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xM1dNKgi8vtF"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Adjust this variable to select the number of training samples\n",
    "train_samples = 5000\n",
    "valid_samples = 60   \n",
    "test_samples = 62  \n",
    "clean_train_dataset = clean_text(train_dataset['text'][:train_samples])\n",
    "clean_valid_dataset = clean_text(valid_dataset['text'][:valid_samples])\n",
    "clean_test_dataset = clean_text(test_dataset['text'][:test_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9zyqmg0eXAI"
   },
   "source": [
    "## Tokenizing and Padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Pq684L6HPIS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKWzxTrcBE42"
   },
   "source": [
    "Apply tokenization and convert all texts to sequences in train, valid and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OseQtmP83Xw5"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_dataset)\n",
    "valid_sequences = tokenizer.texts_to_sequences(clean_valid_dataset)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCkoKE2def-i"
   },
   "source": [
    "This is one of the crucial parameters for training. You can adjust the `max_seq_length` to determine the maximum length of the number of words in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo_cT7At9yWY"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Set maximum sequence length (you can adjust this)\n",
    "max_seq_length = 500\n",
    "\n",
    "# Truncate the length of each sequence upto max_seq_length\n",
    "train_trunc_sequences = truncate_sequences(train_sequences, max_seq_length)\n",
    "valid_trunc_sequences = truncate_sequences(valid_sequences, max_seq_length)\n",
    "test_trunc_sequences = truncate_sequences(test_sequences, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teIp5ozp8-JP"
   },
   "outputs": [],
   "source": [
    "sequence_lengths = [len(seq) for seq in train_trunc_sequences]\n",
    "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
    "print(f\"Average sequence length: {sum(sequence_lengths) / len(sequence_lengths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ayz3TMHhh1X"
   },
   "source": [
    "## Creating Input Output pairs\n",
    "The following method is used to create the input output pairs.\n",
    "```\n",
    "create_input_output_pairs(sequences, max_seq_length):\n",
    "    \"\"\"\n",
    "    Creates input-output pairs from the tokenized sequences. The input will be\n",
    "    subsequences of the original sequence (up to max_seq_length), and the output\n",
    "    will be the next token in the sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequences (List[List[int]]): A list of tokenized sequences.\n",
    "        max_seq_length (int): The maximum sequence length for truncation.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of padded input sequences.\n",
    "        np.array: Array of output words (next token in the sequence).\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9eKkvFb-EZ8"
   },
   "outputs": [],
   "source": [
    "# Create input-output pairs for training\n",
    "\n",
    "train_inputs, train_outputs = create_input_output_pairs(train_trunc_sequences, max_seq_length)\n",
    "valid_inputs, valid_outputs = create_input_output_pairs(valid_trunc_sequences, max_seq_length)\n",
    "test_inputs, test_outputs = create_input_output_pairs(test_trunc_sequences, max_seq_length)\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzoC2j1UG6Ni"
   },
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJCU0TEaMurY"
   },
   "source": [
    "## Model Building\n",
    "Build a `SimpleRNN` model, add hidden layers and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7j5oMxrMZdU"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "def build_rnn_model(vocab_size, max_seq_length):\n",
    "    \"\"\"\n",
    "    Builds an RNN model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        max_seq_length (int): Maximum input sequence length.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Compiled RNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define RNN model\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim=vocab_size, output_dim=100),\n",
    "        # Hidden Layers with Dropout\n",
    "        SimpleRNN(128, return_sequences=False),\n",
    "        Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),  # Dropout layer\n",
    "        # Dense layer for output\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    test_model_structure(model, vocab_size)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyKyv8CYWwsX"
   },
   "source": [
    "### Improvement Strategies\n",
    "\n",
    "Here are some techniques to make the training process more robust and improve performance:\n",
    "\n",
    "* Increase the model capacity by stacking multiple **SimpleRNN** layers.\n",
    "* Change the Embedding size.\n",
    "* Consider adding `Dropout` or `BatchRegularization` layers to converge faster and generalize well.\n",
    "* Increase the dataset size or add more training data. (**Remember:** You can add upto 23k training records)\n",
    "  * You can also consider increasing the `max_seq_length`.\n",
    "* Try different optimization techniques.\n",
    "* Increase the number of epochs.\n",
    "\n",
    "**Test case:** Achieve atleast 25% validation accuracy in order to pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlSwdSrJDWLv"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def main():\n",
    "  import tensorflow as tf\n",
    "  tf.keras.backend.clear_session()   \n",
    "  \n",
    "  vocab_size = len(tokenizer.word_index) + 1\n",
    "  model = build_rnn_model(vocab_size, max_seq_length)\n",
    "  \n",
    "  # Compile\n",
    "  model.compile(loss='sparse_categorical_crossentropy', \n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  # Train with progress\n",
    "  history = model.fit(train_inputs, train_outputs, \n",
    "                      epochs=10,   \n",
    "                      batch_size=512, \n",
    "                      validation_data=(valid_inputs, valid_outputs),  \n",
    "                      verbose=1)   \n",
    "  \n",
    "  # Summary\n",
    "  model.summary()\n",
    "  \n",
    "  # Evaluate\n",
    "  loss, accuracy = model.evaluate(test_inputs, test_outputs, verbose=0)\n",
    "  print(f'\\nTest Accuracy: {accuracy}')\n",
    "  test_validation_accuracy(history)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKnNvnp4qihb"
   },
   "source": [
    "## BLEU Score Evaluation\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate how well a machine-generated text matches a human-written reference text. It's commonly used in tasks like machine translation and text generation. <br> The score ranges from 0 to 1.\n",
    "* 1 means the generated text perfectly matches the reference.\n",
    "* 0 means there's no similarity at all.\n",
    "\n",
    "It checks for both word matches and the correct sequence of words, while penalizing texts that are too short or too long.\n",
    "\n",
    "In order to perform this evaluation,\n",
    "1. We'll be converting the tokens back to words and get the reference words.\n",
    "2. Generate predictions on the input tokens and get the predicted words.\n",
    "3. Then calculate BLEU score by passing the reference and predicted words as input.\n",
    "\n",
    "**Test case**: Achieve Atleast 15% to pass this test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YvmZMT4GG8r"
   },
   "outputs": [],
   "source": [
    "# Convert test outputs to reference words\n",
    "reference_words = convert_token_ids_to_words(test_outputs, tokenizer)\n",
    "\n",
    "# Generate predictions\n",
    "predicted_words = generate_predictions(model, test_inputs, tokenizer)\n",
    "\n",
    "# Calculate BLEU score\n",
    "calculate_bleu(predicted_words, reference_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N5JdEMTGGwh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKd2l64g1huN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFqMGdcY1hsU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFGB7tZz1hny"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2AxYY6DVqu4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
